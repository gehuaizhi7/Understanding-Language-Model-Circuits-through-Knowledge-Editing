{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit the circuit\n",
    "To find the circuit (the important subnetwork), we need to \"mask out\" the other parts of the network. Differential masking uses trainable masks to do so. Following is one convenient way.\n",
    "\n",
    "## Trainable masks \n",
    "This computation graph shows a forward procedure that passes the gradient through the mask without changing the model parameters. Here `w` is the model's parameter, `x` is the input, and `loss` is the output. We use an external \"mask parameter\", `m` that is trainable, to \"cover up\" those model parameters that are not necessary.  \n",
    "(Might need vscode's Mermaid plugin to show the graph).  \n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "\tw -..-> new_w\n",
    "\tnew_w --> Opmask((*))\n",
    "\tm --> Opmask\n",
    "\tOpmask --> Oploss((*))\n",
    "\tx --> Oploss\n",
    "\tOploss --> Loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= tensor(0.5000, grad_fn=<CopyBackwards>)\n",
      "loss= tensor(1.5000, grad_fn=<MulBackward0>)\n",
      "m.grad:3.0, w.grad:None, x.grad: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/6nkzdmxs6hl7xl72dd7_m4cr0000gn/T/ipykernel_76877/589902383.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(\"m.grad:{}, w.grad:{}, x.grad: {}\".format(m.grad, w.grad, x.grad))\n"
     ]
    }
   ],
   "source": [
    "# A minimal example that gets around the \"variable that gradient computation requires is modified in-place\" problem\n",
    "\n",
    "w = torch.tensor(1., requires_grad=False)  # Weight of the model\n",
    "m = torch.tensor(0.5, requires_grad=True)  # Mask\n",
    "x = torch.tensor(3., requires_grad=True)  # Input tensor\n",
    "\n",
    "# Step 1: Copy the model's weight to a (detached) temporary parameter\n",
    "tmp_w = w.detach().clone()\n",
    "\n",
    "# Step 2: Apply the mask there, where p has gradient\n",
    "new_w = m * tmp_w\n",
    "\n",
    "# Step 3: In-place copy the new weight back to the model\n",
    "w.copy_(new_w)\n",
    "print(\"w=\", w)\n",
    "\n",
    "# Now do the regular forward run of the model\n",
    "loss = x * w\n",
    "loss.backward()\n",
    "\n",
    "# You can see the weight does not contain gradients; the gradients goes to the mask\n",
    "print(\"loss=\", loss)\n",
    "print(\"m.grad:{}, w.grad:{}, x.grad: {}\".format(m.grad, w.grad, x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach allows us to set up an external module to mask a pre-trained DNN model (e.g., GPT2) without needing to rewrite the huggingface's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a sparse circuit\n",
    "\n",
    "A circuit consists of a sparse collection of parameters. We need to make most of the masks either 0 or 1.  \n",
    "Section 3 of [Cao etal (2021)](https://arxiv.org/pdf/2104.03514.pdf) proposes a nice method to do so.  \n",
    "We implemented that in `masked_model.py`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply edits on the circuit\n",
    "Now that we have a `MaskedModel`, with the `self.masks` the names of the masked parameters.  \n",
    "In this step, let's fine-tune the circuit. During the back-propagation procedure, if we see a weight is masked (i.e., `m=0`), we avoid it.  \n",
    "TODO: Can this be done without rewriting the pytorch optimizer?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
